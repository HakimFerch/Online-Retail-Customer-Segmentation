{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries the needed libraries.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import zscore\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\asus\\Documents\\Jupyter notebook projects\\Banking project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'OnlineRetail.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-70b6b5d6e171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'OnlineRetail.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Loading the csv dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\AConda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\AConda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\AConda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\AConda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\AConda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OnlineRetail.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('OnlineRetail.csv',encoding='latin1') #Loading the csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #Displaying the head of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Information:\n",
    "\n",
    "* InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "* StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "* Description: Product (item) name. Nominal.\n",
    "* Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "* InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "* UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "* CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "* Country: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #Informations about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()   #number of null values for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Later we will perform RFM analysis for each customer,so it is necessary to have the CustomerID column with no null values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InvoiceDate']=pd.to_datetime(df['InvoiceDate']) #Changing the invoice date data type to date time.\n",
    "df['CustomerID']=df['CustomerID'].astype(str)       #Casting the customer id variable to string type.\n",
    "df.dropna(subset=['Description','CustomerID'],axis=0,inplace=True) #Dropping null values in customerId and Description.\n",
    "df.drop_duplicates(inplace=True) #Dropping duplicates\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory and RFM Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() #Dataframe description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We notice that the minimum value is -80995 which is negative,my guess is that a negative quantity represents goods returned by the buyer.\n",
    "* The quantity data is way spread with and STD of 284.69 and a range of 161990, as we can see thereis a large gap between the 3rd quantile and the maximum.\n",
    "* Same thing with UnitPrice, a very large data range and a narrow interquantile interval and that may due to the fact that The company mainly sells unique all-occasion gifts hence the disparity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of transactions: \", df['InvoiceNo'].nunique())\n",
    "print(\"Number of products bought: \", df['StockCode'].nunique())\n",
    "print(\"Number of customers:\", df['CustomerID'].nunique())\n",
    "print('Number of countries: ', df['Country'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(col): # A function that returns number of values within a categorical feature\n",
    "    return df[col].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_count=df.groupby(['Description'])[['Quantity']].sum() #Total sold quantity for each item\n",
    "items_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot for TOP 5 most sold items\n",
    "fig,ax=plt.subplots(figsize=(12,6))\n",
    "plt.style.use('seaborn')\n",
    "ax.bar(count('Description')[:5].index,count('Description')[:5])\n",
    "ax.set_title('Top 5 most sold items'.title())\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count('Country')/len(df)*100    #percentage of sales by country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 88.95% of the transactions occurred in the United  with Germany and France trailing behind with 2.33% and 2.08%  and that goes back to the fact that the retail is on british soil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the canceled orders percentage\n",
    "\n",
    "df_canceled=df[df['InvoiceNo'].str.contains('C')]\n",
    "total_invoice=df['InvoiceNo'].nunique()\n",
    "cancel_count=df_canceled['InvoiceNo'].nunique()\n",
    "canceled_ratio=cancel_count/total_invoice\n",
    "print('Percentage of orders cancelled: {:.2f}% '.format(canceled_ratio*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canceled=df[df['InvoiceNo'].str.contains('C')]\n",
    "df_canceled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canceled[df_canceled['Quantity']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We notice that negative quantity values are attributed to canceled invoices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Total']=df['Quantity']*df['UnitPrice']\n",
    "per=df['InvoiceDate'].dt.to_period('M')\n",
    "df_trans_date=df.groupby(per)[['InvoiceNo']].count()\n",
    "df_sales_date=df.groupby(per)[['Total']].sum()\n",
    "df_trans_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_date.index=df_trans_date.index.astype('datetime64[ns]')\n",
    "df_sales_date.index=df_sales_date.index.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly total transactions lineplot\n",
    "fig,ax=plt.subplots(2,1,figsize=(12,12))\n",
    "ax[0].plot(df_trans_date.index[:-1],df_sales_date['Total'][:-1],color='r')\n",
    "ax[1].plot(df_trans_date.index[:-1],df_trans_date['InvoiceNo'][:-1],color='b')\n",
    "ax[0].set_xlabel('Invoice Date')\n",
    "ax[0].set_ylabel('Total Sales')\n",
    "ax[1].set_xlabel('Invoice Date')\n",
    "ax[1].set_ylabel('Total Transactions')\n",
    "ax[1].set_title('total sales by month'.title())\n",
    "ax[0].set_title('total transactions by month'.title())\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We notice an upward trend in the number of transactions and sale's income  and it starts at the fall of the year 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including only the positive quantity and unit prices n the dataframe\n",
    "df=df[(df['Quantity']>0) & (df['UnitPrice']>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As i stated previously, the orders with a negatie quantity are canceled, so it is suitable to not include in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFM segmentation technique is the idea that marketers can gain an extensive understanding of their customers by analyzing three quantifiable factors which are Recency(R),Freqency(F) and Monetary(M).\n",
    "\n",
    "* **Recency** is simply the amount of time since the customerâ€™s most recent transaction (most businesses use days, though for others it might make sense to use months, weeks or even hours instead).\n",
    "\n",
    "\n",
    "* **Frequency** is the total number of transactions made by the customer (during a defined period).\n",
    "\n",
    "\n",
    "* **Monetary** is the total amount that the customer has spent across all transactions (during a defined period).\n",
    "\n",
    "The trick is to basically  assign a score to each customer for each value of the three dimensions, then divide the customers into tiered groups using the overall score which is a combination of all three values .\n",
    "\n",
    "There is no conventinal number of levels but it is common to divide them into 4 or 5 groups, which means each dimension can take also 4 or 5 values.\n",
    "\n",
    "Another common way to segment customers is to label specific groups based on their overall score, e.g., 'Best Customers'(R=Highest,F=Highest,M=Highest),'Low spending active customers (R=High,F=High,M=Low)', and for each customers segment, we try to invest in a marketing strategy to imporve their purchase behaviour.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monetary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monetary dataframe\n",
    "rfm_m=df.groupby(['CustomerID'])[['Total']].sum()\n",
    "rfm_m.reset_index()\n",
    "rfm_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Recency dataframe\n",
    "df['days_diff']=df['InvoiceDate'].max()-df['InvoiceDate']\n",
    "rfm_r=df.groupby(['CustomerID'])[['days_diff']].min()\n",
    "rfm_r.reset_index()\n",
    "rfm_r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency dataframe\n",
    "rfm_f=df.groupby(['CustomerID'])[['InvoiceNo']].count()\n",
    "rfm_f.reset_index()\n",
    "rfm_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the dataframes into one\n",
    "rfm=rfm_f.merge(rfm_m,how='inner',on='CustomerID')\n",
    "rfm=rfm.merge(rfm_r,how='inner',on='CustomerID')\n",
    "rfm.columns=['Frequency','Monetary','Recency']\n",
    "RFM=rfm.copy()\n",
    "rfm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing each dimension using quantiles then assigning scores to all 3 RFM factors.\n",
    "rfm['Recency_score']=pd.qcut(rfm['Recency'].sort_values(ascending=True),q=5,labels=[5,4,3,2,1]).astype('int64')\n",
    "rfm['Frequency_score']=pd.qcut(rfm['Frequency'].sort_values(ascending=True),q=5,labels=[1,2,3,4,5]).astype('int64')\n",
    "rfm['Monetary_score']=pd.qcut(rfm['Monetary'].sort_values(ascending=True),q=5,labels=[1,2,3,4,5]).astype('int64')\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall rfm score\n",
    "rfm['rfm_score']=rfm['Recency_score']+rfm['Frequency_score']+rfm['Monetary_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to segment customers into 4 levels (bronze,silver,golden and platinum)\n",
    "def segments(rfm):\n",
    "    \n",
    "    if rfm['rfm_score']<=4:\n",
    "        return 'Bronze Customer'\n",
    "    elif (rfm['rfm_score'] > 4) and (rfm['rfm_score'] <9):\n",
    "        return 'Silver Customer'\n",
    "    elif (rfm['rfm_score'] >=9 ) and (rfm['rfm_score'] <13):\n",
    "        return 'Golden Customer'\n",
    "    else:\n",
    "        return 'Platinum'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm['Segment']=rfm.apply(segments,axis=1)\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping by segments and aggregating by RFM dimensions\n",
    "rfm_seg=rfm.groupby('Segment').agg({'Frequency':'mean','Recency_score':'mean','Monetary':'sum'}).round(1)\n",
    "rfm_seg=rfm_seg.merge(rfm_seg['Monetary']/rfm_seg['Monetary'].sum(),how='inner',on='Segment')\n",
    "columns={'Frequency':'Average_Frequency','Recency_score':'Average_Recency','Monetary_x':'Monetary Total','Monetary_y':'Monetary_ratio'}\n",
    "rfm_seg.rename(columns,axis=1,inplace=True)\n",
    "rfm_seg['Monetary_ratio']=rfm_seg['Monetary_ratio'].round(2)\n",
    "rfm_seg=rfm_seg.sort_values(by=['Monetary_ratio'])\n",
    "rfm_seg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfm_agg=rfm.groupby('rfm_score').agg({'Frequency_score':'mean','Recency_score':'mean','Monetary_score':'mean'}).round(1)\n",
    "rfm_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display value on top of the barchart.\n",
    "def without_hue(plot, feature):\n",
    "    total = len(feature)\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width() / 2 - 0.05\n",
    "        y = p.get_y() + p.get_height()\n",
    "        ax.annotate(percentage, (x, y), size = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot to showcase customers distribution by segments.\n",
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(rfm['Segment'],saturation=0.8,edgecolor='.5')\n",
    "plt.title('Customer segments distribution')\n",
    "without_hue(ax,rfm['Segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(3,1,figsize=(15,14))\n",
    "\n",
    "for col,i in zip(['Recency_score','Frequency_score','Monetary_score'],range(0,3)):\n",
    "    sns.countplot(rfm['Segment'],ax=ax[i],hue=rfm[col],saturation=0.8,edgecolor='.5')\n",
    "    ax[i].set_title(col)\n",
    "    ax[i].legend(bbox_to_anchor=(1,1),title=col)\n",
    "    \n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platinum\n",
    "* Platinum Customers are the company's biggest asset,although they represent 20%, they're the most contributing segment of clients to the total reveue with a precentage of 72%. So it is wise to think of ways to establish a strong bond with them,keep them around and make them feel valued,the key is to make sure they keep on the same behaviour.\n",
    "* One way to approach this is by establishing a personalized communication  to gauge their interest,offering occasional gifts, vip exclusive products or discount codes.\n",
    "\n",
    "### Golden\n",
    "* Despite the strong presence of the golden segment, a good chunk of clients have a RFM scores of 3 or less, which is a risky situation because it's either they're not spending enough, or they've been away for a while, So it is wise to give them a reminder with the lastest discounts,products that may catch their interest based on their latest transactions.\n",
    "* The monetary scores of golden customers revolves mostly around 4 and 3, it's important to motivate them to spend more and buy higher quality items, a cross-selling recommendation system would be highly beneficial in this case, as it suggests newer products of different categories.\n",
    "\n",
    "### Silver and bronze\n",
    "* Silver and bronze segment represent a largest part of our customers , but their monetary value is quite low and they're not frequent enough, they can be just passing by, not that interested in the business's products, or simply lost customers.\n",
    "* It would not be wise to waste resources on marketing strategies that targets them to win back this type of client because they're unlikely to start buying again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the next section, i am going to approach segmenting customers in a different way using KMeans clustering then try to compare both approaches outcomes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RFM['Recency']=RFM['Recency'].dt.days #Using days as unit for recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3,figsize=(16,6))    #RFM Distribution plots\n",
    "for col,i in zip(RFM.columns,range(0,3)):\n",
    "    sns.distplot(RFM[col],ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both frequency and monetary data are heavily skewed, so before feeding it to the model, it's recommended to normalize the distributions and that will help with the performance of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to normalize data using powers inferiot between 1 and 0.\n",
    "\n",
    "interval=np.arange(0.1,0.9,0.1)\n",
    "def unskew_power(df,col):\n",
    "    skew=[]\n",
    "    power=[]    \n",
    "    for i in interval:\n",
    "        skew.append(abs(np.power(df[col],i).skew()))\n",
    "        power.append(i)\n",
    "    table= np.array([skew,power]).T\n",
    "    min_power=table[:,0].min()\n",
    "    return np.power(df[col]+abs(df[col].min()),min_power)\n",
    "     \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unskew_log2(df,col):\n",
    "    return np.log(df[col]+abs(min(df[col]))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to normalize data using Logarithmic tranformation.\n",
    "def unskew_log3(df):\n",
    "    return df.apply(unskew_log).apply(np.log,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3,figsize=(16,6))\n",
    "for col,i in zip(RFM.columns,range(0,3)):\n",
    "    sns.distplot(unskew_power(RFM,col),ax=ax[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3,figsize=(16,6))\n",
    "for col,i in zip(RFM.columns,range(0,3)):\n",
    "    sns.distplot(unskew_log2(RFM,col),ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can all three distribution are much closer to the infamous normal bell curve.The logarithmic function was more efficient as it reduced skewness more than the power function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the log function to the RFM dataframe\n",
    "unskewed_log_rfm=pd.DataFrame({'Frequency':unskew_log2(RFM,'Frequency'),'Recency':unskew_log2(RFM,'Recency'),'Monetary':unskew_log2(RFM,'Monetary')})\n",
    "unskewed_log_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Scaling the data using the standardcaler\n",
    "scaler=StandardScaler()\n",
    "scaled_rfm=scaler.fit_transform(unskewed_log_rfm)\n",
    "scaled_rfm=pd.DataFrame(scaled_rfm,index=unskewed_log_rfm.index,columns=unskewed_log_rfm.columns)\n",
    "scaled_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method plot to determine the optimal number of clusters\n",
    "inertia=[]\n",
    "for i in range(1,15):\n",
    "        KM=KMeans(n_clusters=i,init='k-means++', n_init=10, max_iter=400)\n",
    "        KM.fit(scaled_rfm)\n",
    "        km_iner=KM.inertia_\n",
    "        inertia.append(km_iner)\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(1,15),inertia)\n",
    "plt.xlabel('number of clusters'.title())\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow method')\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The elbow method  suggests that the optimal number of clusters is where the bend occurs,and when we examine the plot, we can clearly notice the change of the slop at n_clusters=2, therefore, 2 is the optimale number of segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette score\n",
    "for i in range(2,15):\n",
    "    KM=KMeans(n_clusters=i,init='k-means++', n_init=10, max_iter=400)\n",
    "    KM.fit(scaled_rfm)\n",
    "    labels=KM.predict(scaled_rfm)\n",
    "    silhouette_avg = silhouette_score(scaled_rfm,labels)\n",
    "    print(\"For n_clusters =\", i,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_clusters with the highest slihouette score is the optimal number of clusters, in our case it is 2 once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting customers for each cluster\n",
    "\n",
    "KM=KMeans(n_clusters=2,init='k-means++', n_init=10, max_iter=50)\n",
    "KM.fit(unskewed_log_rfm)\n",
    "labels=KM.predict(unskewed_log_rfm)\n",
    "unskewed_log_rfm['Label']=labels\n",
    "unskewed_log_rfm['Label'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns a scatter plot highlighting clusters in different colors.\n",
    "\n",
    "def scatter_plot(col1,col2):\n",
    "    plt.scatter(unskewed_log_rfm[unskewed_log_rfm['Label']==0][col1],unskewed_log_rfm[unskewed_log_rfm['Label']==0][col2],color='r',label='0')\n",
    "    plt.scatter(unskewed_log_rfm[unskewed_log_rfm['Label']==1][col1],unskewed_log_rfm[unskewed_log_rfm['Label']==1][col2],color='g',label='1')\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatter_plot('Recency','Monetary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot('Recency','Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot('Frequency','Monetary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans algorithm classified customers into 2 categories we labeled as 0 and 1, and through the plots we can notice some patterns within the scatter plot:\n",
    "\n",
    "* The '0' class are much higher on both the monetary and frequency dimensions, this class's customers are frequent big spenders compared to class1 although we can see some interference of both clusters.\n",
    "\n",
    "* However when it comes to the recency factor, class '1' has a higher density on the upper part of the axis which as a result will increase the mean compared to the other class.\n",
    "\n",
    "* Customers of class 1 are comparatively new customers but generally lack the loyalty and aptiture to spend more, the top section of this cluster needs to be targeted with a marketing strategy to motivate them to keep buy higher value items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregating by the mean while gruping by labels\n",
    "unskewed_log_rfm['Segment']=rfm['Segment']\n",
    "unskewed_log_rfm.groupby(['Label'])[['Frequency','Recency','Monetary']].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The table backs our interpretation of the scatterplots, class '0' has both a high average frequency and monetary values, while the class 1's average recency is higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
